{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3dc42b-d7e3-48bc-acee-eafdd32f9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361735e8-817b-409b-a674-b4d26a2034fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0d6c49-cdfd-472e-9bf6-2ef161c3ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "    print(\"xgboost error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee79b4a-3690-484d-a3b9-a2cd13b17871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Helper methods\n",
    "def normalize_squad_column(df):\n",
    "    if \"Squad\" in df.columns:\n",
    "        pass\n",
    "    elif \"Team\" in df.columns:\n",
    "        df = df.rename(columns={\"Team\": \"Squad\"})\n",
    "    else:\n",
    "        raise ValueError(f\"No Squad/Team column found: {df.columns.tolist()}\")\n",
    "    df[\"Squad\"] = df[\"Squad\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_form_score(last5):\n",
    "    if not isinstance(last5, str):\n",
    "        return 0.0\n",
    "    mapping = {\"W\": 1, \"D\": 0, \"L\": -1}\n",
    "    return sum(mapping.get(ch, 0) for ch in last5.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985d7548-9c5f-4a1c-bbd7-f3175b1bbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Match-level dataset per season\n",
    "def build_match_dataset_for_season(season_dir, season_weight):\n",
    "    season_dir = Path(season_dir)\n",
    "    season_label = season_dir.name\n",
    "\n",
    "    # Team stats\n",
    "    stats_files = list(season_dir.glob(\"*squad_team_merged*.csv\"))\n",
    "    if not stats_files:\n",
    "        raise FileNotFoundError(f\"No *squad_team_merged*.csv in {season_dir}\")\n",
    "    stats = pd.read_csv(stats_files[0])\n",
    "    stats = normalize_squad_column(stats)\n",
    "\n",
    "    # Add recent performance score\n",
    "    if \"Last 5\" in stats.columns:\n",
    "        stats[\"form_score\"] = stats[\"Last 5\"].apply(compute_form_score)\n",
    "    else:\n",
    "        stats[\"form_score\"] = 0.0\n",
    "\n",
    "    # Fixtures\n",
    "    fix_files = list(season_dir.glob(\"*fixtures*.csv\"))\n",
    "    if not fix_files:\n",
    "        raise FileNotFoundError(f\"No *fixtures*.csv in {season_dir}\")\n",
    "    fixtures = pd.read_csv(fix_files[0])\n",
    "\n",
    "    fixtures[\"Home_Goals\"] = pd.to_numeric(fixtures[\"Home_Goals\"], errors=\"coerce\")\n",
    "    fixtures[\"Away_Goals\"] = pd.to_numeric(fixtures[\"Away_Goals\"], errors=\"coerce\")\n",
    "\n",
    "    fixtures[\"goal_diff\"] = fixtures[\"Home_Goals\"] - fixtures[\"Away_Goals\"]\n",
    "    fixtures[\"result\"] = np.where(\n",
    "        fixtures[\"goal_diff\"] > 0, \"H\",\n",
    "        np.where(fixtures[\"goal_diff\"] < 0, \"A\", \"D\")\n",
    "    )\n",
    "    fixtures[\"season\"] = season_label\n",
    "\n",
    "    # Merge home stats\n",
    "    home_stats = stats.add_prefix(\"home_\")\n",
    "    merged = fixtures.merge(\n",
    "        home_stats, left_on=\"Home_Team\", right_on=\"home_Squad\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge away stats\n",
    "    away_stats = stats.add_prefix(\"away_\")\n",
    "    merged = merged.merge(\n",
    "        away_stats, left_on=\"Away_team\", right_on=\"away_Squad\",\n",
    "        how=\"left\", suffixes=(\"\", \"_dupAway\")\n",
    "    )\n",
    "\n",
    "    merged = merged.drop(columns=[\"home_Squad\", \"away_Squad\"], errors=\"ignore\")\n",
    "\n",
    "    # Season weights -> recent seasons have more\n",
    "    merged[\"season_weight\"] = season_weight\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e434b76-edbf-4173-be7a-a0e7689ae0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Full dataset with all available seasons\n",
    "def build_full_dataset(base_dir: str = \"CSV_files\") -> pd.DataFrame:\n",
    "    base = Path(base_dir)\n",
    "    season_dirs = sorted(base.glob(\"Season_*\"))\n",
    "    if not season_dirs:\n",
    "        raise ValueError(\"CSV_files doesn't contain the correct folder.\")\n",
    "\n",
    "    all_dfs = []\n",
    "    n = len(season_dirs)\n",
    "\n",
    "    for i, season_dir in enumerate(season_dirs):\n",
    "        weight = 0.6 + 0.4 * (i / (n - 1)) if n > 1 else 1.0\n",
    "        print(f\"{season_dir.name} -> weight = {weight:.2f}\")\n",
    "        df_season = build_match_dataset_for_season(season_dir, season_weight=weight)\n",
    "        all_dfs.append(df_season)\n",
    "\n",
    "    full = pd.concat(all_dfs, ignore_index=True)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88cf1daf-9b31-4497-98f7-f75d1477d586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season_2020-2021 -> weight = 0.60\n",
      "Season_2021-2022 -> weight = 0.68\n",
      "Season_2022-2023 -> weight = 0.76\n",
      "Season_2023-2024 -> weight = 0.84\n",
      "Season_2024-2025 -> weight = 0.92\n",
      "Season_2025-2026 -> weight = 1.00\n"
     ]
    }
   ],
   "source": [
    "# 4. Data prep + feature engineering\n",
    "full_df = build_full_dataset(\"CSV_files\")\n",
    "full_df = full_df.dropna(subset=[\"result\"])\n",
    "\n",
    "for col in list(full_df.columns):\n",
    "    if col.startswith(\"home_\"):\n",
    "        base = col[5:]\n",
    "        away_col = \"away_\" + base\n",
    "        if away_col in full_df.columns:\n",
    "            if full_df[col].dtype != \"O\" and full_df[away_col].dtype != \"O\":\n",
    "                full_df[f\"diff_{base}\"] = full_df[col] - full_df[away_col]\n",
    "\n",
    "feature_cols = [\n",
    "    c for c in full_df.columns\n",
    "    if c.startswith(\"diff_\") and full_df[c].dtype != \"O\"\n",
    "]\n",
    "\n",
    "X = full_df[feature_cols]\n",
    "y = full_df[\"result\"].astype(str)\n",
    "w = full_df[\"season_weight\"]\n",
    "\n",
    "mask = X.notna().all(axis=1)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "w = w[mask]\n",
    "\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X, y, w, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0c210e-e20a-4d8a-9501-68de94dc68e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg accuracy: 0.5123762376237624\n",
      "RandomForest accuracy: 0.5767326732673267\n",
      "XGBoost accuracy: 0.5420792079207921\n"
     ]
    }
   ],
   "source": [
    "# 5. Training three models: rf, logreg and xgb\n",
    "MODELS = {}\n",
    "\n",
    "# Logistic Regression\n",
    "logreg_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42,\n",
    "    )\n",
    ")\n",
    "logreg_model.fit(X_train, y_train, logisticregression__sample_weight=w_train)\n",
    "print(\"LogReg accuracy:\", logreg_model.score(X_test, y_test))\n",
    "MODELS[\"logreg\"] = logreg_model\n",
    "\n",
    "# RandomForest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    ")\n",
    "rf_model.fit(X_train, y_train, sample_weight=w_train)\n",
    "print(\"RandomForest accuracy:\", rf_model.score(X_test, y_test))\n",
    "MODELS[\"rf\"] = rf_model\n",
    "\n",
    "# XGBoost\n",
    "if HAS_XGB:\n",
    "    # labels to values\n",
    "    label_to_int = {\"A\": 0, \"D\": 1, \"H\": 2}\n",
    "    int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "\n",
    "    y_train_xgb = y_train.map(label_to_int).values\n",
    "    y_test_xgb = y_test.map(label_to_int).values\n",
    "\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train_xgb, sample_weight=w_train)\n",
    "\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    xgb_acc = (y_pred_xgb == y_test_xgb).mean()\n",
    "    print(\"XGBoost accuracy:\", xgb_acc)\n",
    "\n",
    "    MODELS[\"xgb\"] = xgb_model\n",
    "else:\n",
    "    xgb_model = None\n",
    "    label_to_int = None\n",
    "    int_to_label = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25e5790e-04a1-4e89-b226-b87ed19bc17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Predicting\n",
    "def load_latest_team_stats(base_dir: str = \"CSV_files\") -> pd.DataFrame:\n",
    "    base = Path(base_dir)\n",
    "    season_dirs = sorted(base.glob(\"Season_*\"))\n",
    "    latest = season_dirs[-1]\n",
    "    stats_file = list(latest.glob(\"*squad_team_merged*.csv\"))[0]\n",
    "    stats = pd.read_csv(stats_file)\n",
    "    stats = normalize_squad_column(stats)\n",
    "    if \"Last 5\" in stats.columns:\n",
    "        stats[\"form_score\"] = stats[\"Last 5\"].apply(compute_form_score)\n",
    "    else:\n",
    "        stats[\"form_score\"] = 0.0\n",
    "    return stats\n",
    "\n",
    "\n",
    "LATEST_STATS = load_latest_team_stats()\n",
    "\n",
    "\n",
    "def build_feature_row_for_prediction(home_team, away_team):\n",
    "    home = LATEST_STATS[LATEST_STATS[\"Squad\"] == home_team]\n",
    "    away = LATEST_STATS[LATEST_STATS[\"Squad\"] == away_team]\n",
    "\n",
    "    if home.empty:\n",
    "        raise ValueError(f\"Home team '{home_team}' not found in latest stats.\")\n",
    "    if away.empty:\n",
    "        raise ValueError(f\"Away team '{away_team}' not found in latest stats.\")\n",
    "\n",
    "    home = home.reset_index(drop=True)\n",
    "    away = away.reset_index(drop=True)\n",
    "\n",
    "    data = {}\n",
    "    for col in feature_cols:\n",
    "        base = col[5:]\n",
    "        if base in home.columns and base in away.columns:\n",
    "            hv = home[base].iloc[0]\n",
    "            av = away[base].iloc[0]\n",
    "            if pd.api.types.is_numeric_dtype(type(hv)) and pd.api.types.is_numeric_dtype(type(av)):\n",
    "                try:\n",
    "                    data[col] = float(hv) - float(av)\n",
    "                except Exception:\n",
    "                    data[col] = np.nan\n",
    "            else:\n",
    "                data[col] = np.nan\n",
    "        else:\n",
    "            data[col] = np.nan\n",
    "\n",
    "    row = pd.DataFrame([data], columns=feature_cols)\n",
    "    return row\n",
    "\n",
    "\n",
    "def predict_match(home_team, away_team, model_name):\n",
    "\n",
    "    if model_name not in MODELS:\n",
    "        raise ValueError(f\"Unknown model '{model_name}'. Valid: {list(MODELS.keys())}\")\n",
    "\n",
    "    row = build_feature_row_for_prediction(home_team, away_team)\n",
    "    model = MODELS[model_name]\n",
    "\n",
    "    probs = model.predict_proba(row)[0]\n",
    "    classes = model.classes_\n",
    "\n",
    "    if model_name == \"xgb\":\n",
    "        class_labels = [int_to_label[c] for c in classes]\n",
    "    else:\n",
    "        class_labels = list(classes)\n",
    "\n",
    "    prob_map = {cls: float(p) for cls, p in zip(class_labels, probs)}\n",
    "\n",
    "    return {\n",
    "        \"home_team\": home_team,\n",
    "        \"away_team\": away_team,\n",
    "        \"model\": model_name,\n",
    "        \"probs\": {\n",
    "            \"home_win\": prob_map.get(\"H\", 0.0),\n",
    "            \"draw\": prob_map.get(\"D\", 0.0),\n",
    "            \"away_win\": prob_map.get(\"A\", 0.0),\n",
    "        },\n",
    "    }\n",
    "\n",
    "# Averages from current models\n",
    "def predict_ensemble(home_team, away_team, models=(\"logreg\", \"rf\", \"xgb\")):\n",
    "    probs_list = []\n",
    "\n",
    "    for m in models:\n",
    "        if m not in MODELS:\n",
    "            # If XGBoost isn't installed\n",
    "            continue\n",
    "        \n",
    "        pred = predict_match(home_team, away_team, m)\n",
    "        probs_list.append(pred[\"probs\"])\n",
    "\n",
    "    if len(probs_list) == 0:\n",
    "        raise ValueError(\"No valid models found in MODELS for ensembling.\")\n",
    "\n",
    "    # Average\n",
    "    avg_home = np.mean([p[\"home_win\"] for p in probs_list])\n",
    "    avg_draw = np.mean([p[\"draw\"] for p in probs_list])\n",
    "    avg_away = np.mean([p[\"away_win\"] for p in probs_list])\n",
    "\n",
    "    return {\n",
    "        \"home_team\": home_team,\n",
    "        \"away_team\": away_team,\n",
    "        \"models_used\": [m for m in models if m in MODELS],\n",
    "        \"probs\": {\n",
    "            \"home_win\": float(avg_home),\n",
    "            \"draw\": float(avg_draw),\n",
    "            \"away_win\": float(avg_away),\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bfa1170-5798-4158-9d48-cb002350edc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'home_team': \"Nott'ham Forest\",\n",
       " 'away_team': 'Brighton',\n",
       " 'model': 'rf',\n",
       " 'probs': {'home_win': 0.316, 'draw': 0.23, 'away_win': 0.454}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V2 RANDOM FOREST\n",
    "predict_match(\"Nott'ham Forest\", \"Brighton\", model_name=\"rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c69f141-70c8-4df5-8f58-d4220e588ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'home_team': \"Nott'ham Forest\",\n",
       " 'away_team': 'Brighton',\n",
       " 'model': 'logreg',\n",
       " 'probs': {'home_win': 0.1742474750472955,\n",
       "  'draw': 0.30183438566632814,\n",
       "  'away_win': 0.5239181392863763}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V2 LOGREG\n",
    "predict_match(\"Nott'ham Forest\", \"Brighton\", model_name=\"logreg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "750930eb-764e-48a8-b772-711a5420755f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'home_team': \"Nott'ham Forest\",\n",
       " 'away_team': 'Brighton',\n",
       " 'model': 'xgb',\n",
       " 'probs': {'home_win': 0.31825172901153564,\n",
       "  'draw': 0.09828195720911026,\n",
       "  'away_win': 0.5834663510322571}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V2 XGB\n",
    "predict_match(\"Nott'ham Forest\", \"Brighton\", model_name=\"xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "861400a9-683b-42ad-83cd-3106b1962fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'home_team': \"Nott'ham Forest\",\n",
       " 'away_team': 'Brighton',\n",
       " 'models_used': ['logreg', 'rf', 'xgb'],\n",
       " 'probs': {'home_win': 0.269499734686277,\n",
       "  'draw': 0.21003878095847947,\n",
       "  'away_win': 0.5204614967728778}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# V2 FINAL (Averages from predictions)\n",
    "predict_ensemble(\"Nott'ham Forest\", \"Brighton\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
